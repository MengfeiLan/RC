{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval as load\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.models.bert.modeling_bert import BertForTokenClassification\n",
    "from transformers import BertTokenizer, BertConfig, AdamW, get_cosine_schedule_with_warmup\n",
    "from ast import literal_eval as load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', index_col=False, converters={\n",
    "                 'words': load, 'sent_tag': load}).rename(columns={'words': 'text', 'sent_tag': 'labels'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 512\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]\n",
    "ignore_label_id = CrossEntropyLoss().ignore_index\n",
    "\n",
    "\n",
    "def convert(df):\n",
    "    input_ids_ls = []\n",
    "    attention_mask_ls = []\n",
    "    labels_ls = []\n",
    "    for i in range(len(df)):\n",
    "        sents = df.loc[i, 'text']\n",
    "        labels = df.loc[i, 'labels']\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for j in range(len(sents)):  # loop over each sentence\n",
    "            token_tmp = []\n",
    "            for word in sents[j]:\n",
    "                word_tokens = tokenizer.tokenize(word)\n",
    "                token_tmp.extend(word_tokens)\n",
    "            token_tmp.extend([tokenizer.sep_token])\n",
    "            label_ids.extend([ignore_label_id] *\n",
    "                                (len(token_tmp)-1)+[labels[j]])\n",
    "            tokens.extend(token_tmp)\n",
    "\n",
    "        if len(tokens) > maxlen:\n",
    "            tokens = tokens[:maxlen]\n",
    "            label_ids = label_ids[:maxlen]\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        padding_length = maxlen - len(input_ids)\n",
    "        attention_mask = [1]*len(input_ids) + [0]*padding_length\n",
    "        input_ids.extend([pad_token_id] * padding_length)\n",
    "        label_ids.extend([ignore_label_id] * padding_length)\n",
    "        assert len(input_ids) == maxlen\n",
    "        assert len(attention_mask) == maxlen\n",
    "        assert len(label_ids) == maxlen\n",
    "        input_ids_ls.append(input_ids)\n",
    "        attention_mask_ls.append(attention_mask)\n",
    "        labels_ls.append(label_ids)\n",
    "    tokenized_df = pd.DataFrame(\n",
    "        [input_ids_ls, attention_mask_ls, labels_ls]).transpose()\n",
    "    tokenized_df.columns = ['input_ids',\n",
    "                            'attention_mask', 'labels']\n",
    "    return tokenized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df['paper'].unique()\n",
    "bound = int(0.9*len(ids))\n",
    "train_ids, eval_ids = ids[30:], ids[:30]\n",
    "train_df = df.set_index(\"paper\").loc[train_ids].reset_index()\n",
    "eval_df = df.set_index(\"paper\").loc[eval_ids].reset_index()\n",
    "train_df = train_df.sample(frac=1, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = Dataset.from_pandas(convert(train_df))\n",
    "tokenized_eval = Dataset.from_pandas(convert(eval_df))\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_eval.set_format(\"torch\")\n",
    "train_loader = DataLoader(tokenized_train, shuffle=True, batch_size=4)\n",
    "eval_loader = DataLoader(tokenized_eval, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1(ref, pred):\n",
    "    tp = fn = fp = 0\n",
    "    for i in range(len(pred)):\n",
    "        for j in range(len(pred[i])):\n",
    "            if pred[i][j] == 1 and ref[i][j] == 1:\n",
    "                tp += 1\n",
    "            elif pred[i][j] == 1 and ref[i][j] == 0:\n",
    "                fp += 1\n",
    "            elif pred[i][j] == 0 and ref[i][j] == 1:\n",
    "                fn += 1\n",
    "    pc = rc = f1 = 0\n",
    "    if tp != 0:\n",
    "        pc = tp/(tp+fp)\n",
    "        rc = tp/(tp+fn)\n",
    "        f1 = 2*pc*rc/(pc+rc)\n",
    "    print(f'precision: {pc}, recall: {rc}, F1: {f1}')\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# adapted from https://github.com/DHPO/GHM_Loss.pytorch\n",
    "class GHM_Loss(nn.Module):\n",
    "    def __init__(self, bins, alpha, ignore_label_id=-100):\n",
    "        super(GHM_Loss, self).__init__()\n",
    "        self._bins = bins\n",
    "        self._alpha = alpha\n",
    "        self._last_bin_count = None\n",
    "        self._ignore_label_id = ignore_label_id\n",
    "\n",
    "    def _g2bin(self, g):\n",
    "        return torch.floor(g * (self._bins - 0.0001)).long()\n",
    "\n",
    "    def _custom_loss(self, x, target, weight):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _custom_loss_grad(self, x, target):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        valid_label = (target != self._ignore_label_id)\n",
    "        x = x[valid_label]\n",
    "        target = target[valid_label]\n",
    "\n",
    "        g = torch.abs(self._custom_loss_grad(x, target)).detach()\n",
    "        bin_idx = self._g2bin(g)\n",
    "        bin_count = torch.zeros((self._bins), device=x.device)\n",
    "        for i in range(self._bins):\n",
    "            bin_count[i] = (bin_idx == i).sum().item()\n",
    "        N = x.size(0)  # N = (x.size(0) * x.size(1))\n",
    "        if self._last_bin_count is None:\n",
    "            self._last_bin_count = bin_count\n",
    "        else:\n",
    "            bin_count = self._alpha * self._last_bin_count + \\\n",
    "                (1 - self._alpha) * bin_count\n",
    "            self._last_bin_count = bin_count\n",
    "        nonempty_bins = (bin_count > 0).sum().item()\n",
    "        gd = bin_count * nonempty_bins\n",
    "        gd = torch.clamp(gd, min=1)  # min=0.0001\n",
    "        beta = N / gd\n",
    "        return self._custom_loss(x, target, beta[bin_idx])\n",
    "\n",
    "\n",
    "class GHMC_Loss(GHM_Loss):\n",
    "    def __init__(self, bins, alpha):\n",
    "        super(GHMC_Loss, self).__init__(bins, alpha)\n",
    "\n",
    "    def _custom_loss(self, x, target, weight):\n",
    "        criterion = CrossEntropyLoss(reduction='none')\n",
    "        loss = criterion(x, target)\n",
    "        loss = torch.mean(loss * weight)\n",
    "        return loss\n",
    "\n",
    "    def _custom_loss_grad(self, x, target):\n",
    "        return torch.gather(F.softmax(x, dim=-1).detach(), 1, target.view(-1, 1)) - 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device=device):\n",
    "    model.eval()\n",
    "    val_true, val_pred = [], []\n",
    "    i = 0\n",
    "    eval_loss = 0\n",
    "    for batch in data_loader:\n",
    "        i += 1\n",
    "        mlen = batch['attention_mask'].sum(1).max().item()\n",
    "        batch = {k: v[:, :mlen].to(device) if v.dim()==2 elseÂ v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            eval_loss += criterion(logits.view(-1, 2),\n",
    "                                   batch['labels'].view(-1)).item()\n",
    "        y_pred = torch.argmax(logits, dim=-1).detach().cpu().numpy().tolist()\n",
    "        y_true = batch['labels'].detach().cpu().numpy().tolist()  # .squeeze()\n",
    "        real_len = torch.sum(batch['attention_mask'],\n",
    "                             1).detach().cpu().numpy().tolist()\n",
    "        for j in range(len(real_len)):\n",
    "            pred_tmp = []\n",
    "            true_tmp = []\n",
    "            for k in range(real_len[j]):  # range(len(y_true[j])):\n",
    "                if y_true[j][k] != ignore_label_id:\n",
    "                    pred_tmp.append(y_pred[j][k])\n",
    "                    true_tmp.append(y_true[j][k])\n",
    "            val_true.append(true_tmp)\n",
    "            val_pred.append(pred_tmp)\n",
    "    eval_loss = eval_loss / (i+1)\n",
    "    f1 = F1(val_true, val_pred)\n",
    "\n",
    "    return f1, eval_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, train_loader, valid_loader, criterion,\n",
    "                   optimizer, scheduler, num_epochs, device=device):\n",
    "    best_acc = 0.0\n",
    "    # patience = 0\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    for epoch in range(num_epochs):\n",
    "        count = 0\n",
    "        model.train()\n",
    "        print(\"***** Running training epoch {} *****\".format(epoch+1))\n",
    "        running_loss = 0\n",
    "        for batch in train_loader:\n",
    "            mlen = batch['attention_mask'].sum(1).max().item()\n",
    "            batch = {k: v[:, :mlen].to(device) if v.dim()==2 elseÂ v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits.view(-1, 2), batch['labels'].view(-1))\n",
    "            loss /= grad_acu_steps\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            count += 1\n",
    "            if count % grad_acu_steps == 0:\n",
    "                if count % (grad_acu_steps*20) == 0:\n",
    "                    running_loss /= 20\n",
    "                    print(f'running_loss:{running_loss}')\n",
    "                    running_loss = 0\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "        model.eval()\n",
    "        acc, eval_loss = evaluate(model, valid_loader, criterion)\n",
    "        print(f'evaluation loss: {eval_loss}')\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"current acc is {:.4f}, best acc is {:.4f}\".format(acc, best_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_acu_steps = 4 \n",
    "num_epochs = 8\n",
    "num_training_steps = num_epochs*len(train_loader)//grad_acu_steps\n",
    "\n",
    "config = BertConfig.from_pretrained(\n",
    "    model_path, max_position_embeddings=maxlen, num_labels=2)  # 18\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_path, config=config, ignore_mismatched_sizes=True).to(device)\n",
    "#optimizer = AdamW([{'params': model.bert.parameters()},\n",
    "                   #{'params': model.classifier.parameters(), 'lr': 5e-5}], lr=1e-5)\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.bert.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.bert.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "{'params': [p for n, p in model.classifier.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01, 'lr': 1e-4},\n",
    "{'params': [p for n, p in model.classifier.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0, 'lr': 1e-4},\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=200,\n",
    "                                            num_training_steps=num_training_steps, num_cycles=0.5)\n",
    "criterion = GHMC_Loss(bins=10, alpha=0.9)\n",
    "\n",
    "train_and_eval(model, train_loader, eval_loader,\n",
    "               criterion, optimizer, scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfbfb98cec452e6f7bf3984988d7307b341cb2d8ec51eb959d009f585a229255"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('hl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
